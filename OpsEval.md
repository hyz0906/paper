

这篇论文介绍了OpsEval,这是一个面向大型语言模型的综合性、任务导向的人工智能运维(AIOps)基准测试。主要内容总结如下:

1. OpsEval是第一个全面、任务导向的、双语(英汉)的AIOps基准测试,用于评估大型语言模型在AIOps方面的能力。它涵盖了有线网络运维、5G通信运维和数据库运维三个关键场景。
2. OpsEval的问题集分类覆盖了知识回忆、分析思维和实际应用三个不同的能力层级,以及八种不同的具体运维任务类型。
3. OpsEval包含了7200个客观题(多项选择题)和200个主观题(问答题),可以全面评估语言模型在不同场景和任务上的表现。
4. 论文通过量化和定性结果展示了不同的语言模型技巧(零样本、链式思维、小样本学习等)如何影响AIOps性能。发现GPT4分数与专家评价更加一致,可以替代Bleu和Rouge用于大规模的定性评估。
5. 论文建立了一个基于Docker的评测平台,研究人员可以提交模型进行自动评测,获得定量分数和排名。
6. OpsEval为优化和改进面向AIOps的大型语言模型提供了指导,是这个领域的首个开创性工作。

总体来说,这篇论文设计了一个非常全面和系统的AIOps基准测试,可以有效评估大型语言模型在这个重要应用领域的能力,为语言模型在AIOps方面的研究和应用提供了宝贵基础。这项工作的意义重大且令人印象深刻。





这篇论文的主要内容:

标题:OpsEval:面向大型语言模型的全面任务导向人工智能运维基准测试

摘要: 近年来,大型语言模型(LLM)在NLP相关任务上展现出了显著的能力,如翻译、总结和生成。LLM在特定领域的应用,尤其是人工智能运维(AIOps),由于其在信息摘要、报告分析和API调用能力方面的进步,具有巨大的潜力。然而,当前LLM在AIOps任务上的性能尚未确定。此外,需要一个全面的基准测试来引导优化面向AIOps的LLM。与现有基准测试更侧重于评估特定领域如网络配置的能力不同,本文提出了OpsEval,这是一个面向任务的全面AIOps基准测试,用于评估LLM。OpsEval首次评估了LLM在三个关键场景(有线网络运维、5G通信运维和数据库运维)不同能力层级(知识回忆、分析思维和实际应用)的熟练程度。该基准测试包含7200个英语和中文的多项选择题和问答题。通过定量和定性结果,我们展示了各种LLM技巧如零样本、链式思维和小样本学习对AIOps性能的影响。我们发现GPT4得分与专家相比更加一致,可以替代广泛使用的Bleu和Rouge,用于大规模的定性评估。

简介: 介绍了LLM在NLP任务上的进步,以及它在AIOps领域的应用潜力。指出目前LLM在AIOps任务上的表现不明确,需要一个全面的基准测试来引导LLM的优化。介绍了现有基准测试的局限性,提出了OpsEval来全面评估LLM在AIOps不同场景和能力上的表现。

相关工作: 总结了现有的LLM基准测试,分为通用能力基准和特定领域基准两类。通用能力基准评估LLM的通用能力,特定领域基准评估LLM处理特定领域任务的能力。指出当前还缺乏面向运维领域的基准测试。

OpsEval基准测试: 介绍了OpsEval的设计,包含客观题和主观题。客观题来源于相关认证考试,进行了筛选和标准化处理。主观题部分通过客观题转换以及从书籍中提取生成。题目分类包括8种任务类型和3个能力层级。使用不同的评价指标,客观题用准确率,主观题用Rouge、Bleu、GPT4评分和专家评价。

实验设计: 评估了各种不同规模的LLM,使用零样本、小样本、链式思维等不同设置。同时评估了不同量化参数的LLM。

结果和讨论: 展示了不同LLM在OpsEval上的整体表现。分析了在不同设置、不同任务类型、不同语言及不同量化参数下的表现。发现GPT4-分数与专家评价更加相关。

结论: OpsEval提供了一个全面、任务导向的AIOps基准测试。评估结果可以指导LLM在此领域的优化。该基准测试设计灵活,可以扩展更多细粒度的任务,为LLM在AIOps方面的研究和应用奠定基础。

总体来说,这篇论文设计出一个系统和全面的基准测试来评估LLM在AIOps领域的能力,研究结果对于推动LLM在这个重要领域的发展具有重要意义。





第二章相关工作的翻译如下:

2 相关工作

随着LLM的快速发展,它们日益复杂和多样的能力越来越受到认可。然而,传统的NLP评价指标难以准确地评估这些能力。因此,设计面向LLM的评估基准测试的趋势日益增长。这些基准测试可以大致分为两类:通用能力基准测试和特定领域基准测试。

通用能力基准测试用于评估LLM在各种任务中的通用能力。这些任务测试常识、通用知识、推理能力等,而不限于特定领域。HELM采用7项不同指标,在42种独特场景中进行评估,全面评估了LLM在多个维度上的能力。BIG-bench包含204项任务,涵盖广泛的主题,特别关注当前LLM难以达到的任务。C-Eval是第一个全面中文评估套件,旨在严格评估中文LLM的高级知识和推理能力,以及对中文语言环境下特定知识的理解。AGIEval整理了来自大学入学考试和SAT等考试的真实问题,构建了一个基本以人为本的评估数据集。与此相似,MMCU利用中国高考和各类专业资格考试的问题建立了一个强大的基准测试,专门用于评估理解能力。CG-Eval关注评估LLM的生成能力,其测试框架包括术语定义、简答题和计算问题。

特定领域基准测试则评估LLM处理特定领域任务的能力。这些基准测试通常要求LLM在某一领域拥有专业知识,并以该领域的认知模式回应。尽管LLM在专业领域取得了快速发展,但这些具体领域的评价指标获得的关注要少得多。

FinEval是针对中文LLM的高级金融知识设计的基准测试。MultiMedQA是一个大规模的医学问答数据集,其问题来源于专业医学考试、研究和问诊记录,需要深刻理解医学知识,包括多项选择题和开放式问题。Huatuo-26M和CMB是综合的医学问答数据集。Huatuo-26M包含大量真实医患问诊记录和医学知识问答内容。CMB包括资格考试多项选择题(CMB-Exam)和基于真实病例的复杂临床诊断问题(CMB-Clin),正确答案是通过专家共识确定的。

NetOps关注网络领域的评估,与Ops领域相关。NetOps包含英文和中文的多项选择题,以及少量填空题和问答题。

与现有工作相比,我们的研究涵盖了有线网络运维、5G通信运维和数据库运维等多种Ops子领域。此外,我们仔细划分了任务和能力的分类,从而提供了更加细致全面评估框架。我们也采用了更广泛的更合理的指标来评估主观题,并对LLM在各种任务和能力上的表现进行了详细分析。此外,我们探究了不同量化LLM参数对处理AIOps任务性能的影响。



第三章OpsEval基准测试的翻译如下:

3 OpsEval基准测试

在我们对AIOps领域LLM的评估中,我们将评估问题分类为客观题和主观题。

客观题通常以多项选择题的形式提出,提供了一种具有确定性答案的结构化方法。这些问题简单直接,为评估提供了清晰的指标。然而,鉴于高级模型的复杂性,它们可能会受提供的选项的影响,这可能导致它们的回应更多地由模式识别而不是对内容的适当理解所驱动。

主观题没有预定义的选项。这就需要模型更多地依赖对其理解和知识基础的运用,为洞察其认知能力提供了更清晰的视角。这种问题可以更好地评估LLM生成连贯且与上下文相关的回应的能力。

通过融合两种问题,我们旨在实现全面平衡的评估。这可以确保被评估的模型不仅能识别模式,还能展示对各种任务的真正理解。

3.1 客观题

数据来源:客观题源自AIOps领域享有盛誉的国际认证考试相关的考题,这些考试在全球范围内备受推崇。我们的搜集过程涉及从各种书籍、网上资源和合作机构收集问题。这些问题主要格式为多项选择题,包括单选和多选样式。每个问题都有一个提示、可能的答案选择和相关的解释或分析。我们关注的主要场景包括网络运维、通信技术运维和数据库运维。我们还将继续完善和扩展评估场景。

数据处理:我们对原始测试集进行了系统的多阶段处理:

(1) 初筛:在这一阶段,我们旨在过滤与AIOps无直接关系的问题。为协助这一过程,我们采用GPT-3.5,利用其高级功能确定每个问题与AIOps的相关性。

(2) 重复删除:识别并删除任何重复或高度相似的问题,以避免测试集中存在冗余。

(3) 内容过滤:我们排除主要依赖文章或外部内容的问题。

(4) 格式标准化:我们简化和标准化每个问题的格式。采用的格式定义为元组(Q,A),其中Q表示具有选项的问题,A包括正确答案及其分析。

(5) 人工审核:进行后续的手动筛选,以确保测试集的质量和相关性。这一细致的过程最终导致得到约7000个客观问题的精炼测试集,其中55.7%与有线网络运维相关,37.1%与5G通信运维相关,7.2%与数据库运维相关。

任务分类:在复杂的运维和维护环境中,认识任务和挑战的多维性是必要的。为了全面评估AIOps领域内LLM的能力,我们设计了任务分类,捕获专业人员在实际应用中面临的许多任务。我们明确的八种运维任务的制定受行业相关性、任务频率和每个区域在AIOps中的重要性的影响。八个方案的细节见附录A.1。客观问题在这八个类别中的分布如图1所示。

能力分类:对模型的全面评估不仅要评估其产生正确答案的能力,也要了解其推理在不同认知需求层面的深度、复杂性和适用性。基于回答问题所需的能力,我们手动将所有问题分类为三个类别。这三种能力分别是知识回忆、分析思维和实际应用,反映专业人员在现实世界场景中可能遇到的挑战。三种能力的详细信息见附录A.2。

3.2 主观题

数据收集:OpsEval测试集中的主观问题来源于精心策划的资源组合,以确保全面性和相关性:

- 来自客观题的生成:我们主观题的一部分是从原始测试集中精心选择的客观题转换而来的,这些问题被识别出具有潜在的广度和深度。
- 从书籍中提取:为了增强测试集的多样性和深度,我们还从涵盖各种AIOps领域的权威书籍中提取主观题。这可以确保我们的测试集广泛且与行业标准和当前最佳实践保持一致。

数据处理: 收集后,遵循严格流程对主观题进行细化和标准化:

(1) 问题概述:转化为主观题的客观题进行了概述过程。这涉及提炼每个问题的本质,并以开放式格式呈现,没有预定义的选项。

(2) 包含参考文本: 对于在GPT-4的帮助下生成的问题,在提示中提供参考文本以指导生成过程并确保准确性。

(3) 数据结构化:每个主观题都经过精心组织以包含原始问题、答案的关键点、详细答案、任务和相关能力。这种结构化方法有利于轻松评估和分析。

总体上,我们积累了200个主观题的集合,确保了在AIOps领域对模型能力的全面评估。附录A.3中给出了保存问题的示例。

3.3 评价指标

我们为客观题和主观题使用不同的指标。

对于客观题,我们使用准确率作为指标。由于LLM可能会输出多于选项的内容,我们使用基于正则表达式的选项提取器从LLM的原始回复中提取答案。根据提取的答案和真实标签,我们计算准确率。客观题的基线准确率低于25%,因为我们的问题中包含多选多项选择题。

对于主观题,我们使用两类指标,一类基于词汇重叠,另一类基于语义相似度。对于第一类,我们使用在NLP任务(尤其是翻译任务)中广泛使用的Rouge和Bleu。对于第二类,我们使用GPT-4和专家为LLM的输出得分,在OpsEval中称为GPT4-Score和Expert-Evaluation。

Rouge是用于自动摘要和机器翻译评估的一组指标。ROUGE-N是系统和参考摘要之间n-gram的重叠。ROUGE-L自然地考虑了句子级结构相似性,并自动识别出现最长的共现序列n-gram。简而言之,Rouge可以理解为地面真值答案的召回率。我们使用rouge_score python包在OpsEval中计算Rouge1、Rouge2和RougeL。Rouge的分数规范化为0到100。分数越高越好。

Bleu可以理解为生成答案的精确度。我们在OpsEval中使用scarebleu python包计算Bleu。Bleu的分数规范化为0到100。分数越高越好。

GPT4-Score是GPT4用精心设计的提示生成的分数。随着LLM参数的增大,基于LLM的评分越来越广泛使用。我们组合问题、答案要点、详细答案和LLM的答案的提示进行评分。分数在1到10之间,分数越高越好。

Expert-Evaluation是我们根据三个与运维需求高度相关的标准手动对LLM输出进行评分而设计的。考虑的三个标准如下:

- 流畅性。评估模型输出中的语言流畅性,遵循主观题答题要求的情况,以及段落重复或无关文本的出现或缺失。
- 准确性。评估模型输出的精确性和正确性,包括其是否足够涵盖地面真值答案的要点。
- 证据。检查模型输出是否包含足够的论证和证据支持以确保答案的可信度和可靠性。

对于LLM的每一个问题的输出,我们要求专家根据每一项标准对其打分0到3分。在评分过程中,每轮给出原始问题、答案要点、详细答案和一个匿名模型的输出。由于避免偏见对结果产生影响至关重要,不提供关于匿名模型的任何信息。



第四章实验设计的翻译如下:

4 实验设计

在本节中,我们将展示OpsEval的实验设计。我们在OpsEval上评估各种LLM,目的是了解不同LLM在处理不同类型的问题(客观题和主观题)和任务时的多种能力。我们还评估了具有不同量化参数的LLM。

4.1 模型

如表1所示,我们评估了各种流行的LLM,这些LLM可以处理英文和中文输入。表1中所有LLM的详细信息可在附录B.1中找到。

此外,我们评估了具有多个量化参数的LLaMA-2-70B,以对不同量化参数的效果获得概览。具体而言,我们使用3位和4位量化参数的GPTQ模型。GPTQ模型评估的详细信息可在表2中找到。

4.2 设置

4.2.1 客观题

为了全面了解流行LLM在OpsEval上的表现,我们尽可能多地使用设置进行评估。我们在零样本和小样本设置(实验中为3样本)下评估LLM。对于零样本设置,我们希望从用户的角度评估LLM的能力,因为在普通使用中,用户不会提供示例。对于小样本设置,我们旨在从开发者的角度评估LLM的潜力,这可以比零样本设置获得更好的表现。对于每种设置,我们用四种提示工程子设置对LLM进行评估,即朴素答案(Naive)、自我一致(SC)、链式思维(CoT)和链式思维与自我一致(CoT+SC)。由于我们有英文和中文问题,我们为两种语言设计提示。

朴素:朴素设置希望LLM在不需要任何其他解释的情况下生成答案。由于我们知道每个问题的任务类型,我们将任务集成到提示中。

SC:自我一致是在对LLM进行几次查询后选择最一致的答案。尽管它旨在“替代链式思维提示中使用的贪心解码”,但它可以像CoT一样生成朴素答案,因为它可能对相同的提示生成不同的答案。我们在SC中将查询次数设置为5。

CoT:CoT设置旨在通过中间推理步骤为LLM提供复杂的推理能力。我们为零样本和小样本评估构建了特定的CoT提示。提示构建的详细信息可在B.1中找到。

CoT+SC:我们结合CoT和SC以提升CoT提示的性能。通过SC,我们选择一致的推理路径和答案进行几次相同的查询。与SC设置类似,我们在CoT+SC中将查询次数设置为5。

4.2.2 主观题

我们将每个问题的场景和能力与问题本身组合成提示,要求LLM根据问题生成答案。由于我们希望模拟日常用户对LLM的使用,并期望LLM根据问题生成答案,我们在朴素设置下对LLM进行零样本评估。构建提示的示例可在附录A.3中的图10中找到。



第五章评估的翻译如下:

### 5 评估

5.1 整体性能

有线网络运维测试集上零样本和小样本评估四个设置下的结果如表3所示。其他两个测试集上的结果在附录B.3中给出。表中呈现了英语和中文问题的性能。

从整体性能结果中,我们可以得出几点发现:

在英语和中文问题上,GPT-4始终优于所有其他模型,超过了所有其他LLM的最佳性能。在有线网络运维测试集上,当采用自我一致和链式思维提示时,LLaMA-2-13B和百川-13B-Chat的性能在英语和中文测试集上都接近ChatGPT。

小型模型(如LLaMA-2-7B和InternLM-7B)在客观题中表现出竞争力,接近拥有130亿参数的模型的能力,这归功于它们的微调过程和训练数据质量。

此外,四种提示设置的效果因不同LLM而异。经过专门针对中文微调的LLM无论在英文还是中文测试集上表现较好,与未进行中文微调的LLM相比。我们在5.6节中对这些观察结果进行了进一步分析。

5.2 不同设置下的性能

对于英文和中文测试集,我们检查LLM在四种上述设置下的零样本和小样本性能。结果如图2和图3所示。基于图2(以及其他测试集的结果),我们可以得出以下结论:

对于大多数模型,从朴素设置到SC、CoT和SC+CoT,性能有所改善。小样本性能优于零样本性能。

在这些设置中,CoT提示导致LLM回答能力的最显著提高。SC提示导致的改善相对较小,因为LLM对重复问题的回答倾向于一致,这与运维任务中可靠性和一致性的期望一致。

在极少数情况下,更高级的评估方法令人惊讶地导致较差的结果。对此的详细分析见附录B.5.1。

5.3 不同任务和能力上的性能

为了研究LLM在每个运维任务中的表现,以及它们在什么程度上拥有知识回忆、分析思维和实际应用的能力,我们根据3.1节提到的任务和能力分类,总结了不同参数规模的LLM组的结果,并在两个雷达图上绘制,分别关于它们的能力表现和任务表现,如图6所示。

在我们测试的八项任务中,LLM在通用知识任务中总体上取得更高的准确率,而在高度专业化的任务(如自动化脚本和网络配置)中的表现则出现明显的下降和波动,这反映了专业语料库和领域知识对LLM表现的影响。

在三种能力中,LLM在实际应用上表现最好,其次是知识回忆。LLM在分析思维问题上表现较差是可以预期的,因为准确推断现有事实的结论对LLM仍是一个有挑战的研究课题。LLM在实际应用上表现最好,因为我们测试的LLM是在包含最佳运维实践的语料库上训练的,这使LLM熟悉许多实际任务的解决方案。

通过按参数规模对LLM进行分组,我们发现尽管130亿参数的LLM在最佳情况下的准确率高于不超过70亿参数的LLM,但不同的130亿LLM之间的表现差异很大,导致其下限甚至低于70亿参数的LLM。另一方面,参数不超过70亿的LLM在组内有更稳定的性能范围。

5.4 主观题性能

表4按GPT4得分排序显示了200个英文网络运维主观题在四个指标(Rouge、Bleu、GPT4得分和专家评价)上的评估结果。

基于Rouge和Bleu得分的排名与基于GPT4得分和专家评价的排名不太吻合,如表5所示。实际表现较差的LLM可能生成关键词,从而获得更高的Rouge和Bleu分数。相反,表现良好的LLM由于措辞与标准答案不同,可能获得较低的Rouge/Bleu分数。

关于GPT4得分,排名与基于专家评价的排名高度吻合。在表6中,我们计算了GPT4得分与专家评价各子指标的相关系数,以进一步洞察。在三个指标中,GPT4得分的排名与准确性指标最接近,表明GPT4凭借其广泛的知识库在事实性方面最可靠。生成内容的格式和长度也严重影响GPT4得分,正如与流畅性的高正相关所暗示的。另一方面,关于证据指标存在更多差异,表明GPT4得分需要充分考虑论点和证据在答案不明确的情况下的作用。

在专家评价中,证据是一个重要标准,像中文Alpaca-2-13B这样在答案中使用更充分论证的LLM即使准确性得分远低于ChatGPT,也可以在总分上胜过后者。

这里,我们通过给流畅性、准确性和证据指标相同权重计算专家评价总分,这应仅被视为评估这些方面的众多方法之一。根据实际场景,应基于实际情况为专家评价的子指标分配不同权重。

5.5 不同量化参数的性能

图7显示了不同量化参数的LLaMA-2-70B在英文客观题上的准确率。我们进行了零样本和小样本评估,使用朴素设置。推理中使用量化会降低LLM的性能。

LLaMA2-70B-Int4可以达到与无量化的LLaMA-2-70B相近的准确率。具体来说,在英文客观题的零样本评估中,4位量化参数的GPTQ模型的准确率比LLaMA-2-70B低3.50%,在小样本评估中低0.27%。对于中文题,LLaMA2-70B-Int4的准确率比LLaMA-2-70B的零样本评估低3.67%,小样本评估低5.18%。

然而,LLaMA2-70B-Int3的性能下降是不可忽略的,如图7所示。平均而言,LLaMA2-70B-Int3的准确率比LLaMA-2-70B低12.46%,比LLaMA2-70B-Int4低9.30%。原因可能是3位量化中丢失了完整模型过多信息。

5.6 不同语言上的性能

在图5中,我们比较了各种LLM在CoT+SC设置下对英文和中文题的小样本性能。值得注意的是,一些经过专门针对中文进行训练或微调的LLM,如中文Alpaca-2-13B、Qwen-7B-Chat和ChatGLM2-6B,在回答英文题时的表现仍优于中文题。

我们分析了这一现象的潜在原因如下:

(1) 一些LLM可能已经在我们测试集的英文来源书籍上进行过训练,这会给它们在回答英文题上带来天然优势。

(2) 在将英文转为中文的过程中,某些问题的上下文可能由于翻译软件和人工审查的限制而发生偏差,从而影响中文题质量。

(3) 经过专门针对中文微调的LLM中文回答能力可能有所改善,但它们可能在理解问题格式和与CoT相关的提示理解方面出现退步。

尽管我们观察到从英文题转为中文题时性能往往会降低,但我们仍可以从中获得LLM语言能力的宝贵见解。特别是:

(1) ChatGLM2-6B在转向中文题时性能下降最小。这可以归因于其在训练过程中而不是在现有基础模型之上进行简单微调时大量接触中文语料。

(2) LLaMA-2-13B在切换到中文题时表现下降最明显。这表明语言的改变影响了LLM的一般理解能力和提取特定领域知识的能力。

我们还观察到百川-13B-Chat在CoT+SC设置下的3样本评估中一个有趣的现象,其在中文题上的表现明显优于英文题。我们检查LLM的输出并分析一个示例问题,以阐明这一现象,详见附录B.5.2。

### 6  结论

本文介绍了OpsEval,这是一个面向LLM的全面任务导向的人工智能运维基准测试。OpsEval前所未有地评估了LLM在三个关键场景(有线网络运维、5G通信运维和数据库运维)不同能力层次(知识回忆、分析思维和实际应用)的熟练程度。该基准测试包含7200个英语和中文的多项选择题和问答题。

通过定量和定性结果,我们详细阐释了零样本、链式思维和小样本学习等各种LLM技巧对AIOps性能的细微影响。值得注意的是,GPT4得分与广泛使用的Bleu和Rouge相比更为可靠,表明其可以替代大规模定性评估中的自动指标。

OpsEval的确定性框架提供了未来探索的灵活性。该基准测试可以无缝集成更多细粒度的任务,为继续研究和优化面向AIOps的LLM奠定基础。
